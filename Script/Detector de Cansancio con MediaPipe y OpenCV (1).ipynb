{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 100; padding-top: 22px; text-align:right;\">Jokin Cuesta Arrillaga</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "# IA: DETECTOR DE CANSANCIO CON MEDIAPIPE Y OPENCV\n",
    "\n",
    "Autor: Jokin Cuesta Arrillaga.\n",
    "\n",
    "**Importante:** este proyecto se ha realizado en base al vídeo: *AI Body Language Decoder with MediaPipe and Python in 90 Minutes* de Nicholas Renotte. El proyecto se ha llevado a cabo con el único fin de desarrollar habilidades y en **ningún caso ha sido para obtener ningún beneficio económico.**\n",
    "\n",
    ">A lo largo de este proyecto se utilizará  Mediapipe para estimar puntos de referencia tanto faciales como corporales, obtenidos directamente de una WebCam a través de la OpenCV . Con esos datos, se crearán modelos personalizados de clasificación de poses que le permitan decodificar lo que una persona podría estar diciendo con su lenguaje corporal con gran precisión, adaptando a las necesidades de cada aplicación. En este proyecto en concreto, se tratará de realizar detección de somnolencia para conductores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "**Antes de nada, instalaremos todas las dependencias que necesitaremos en este proyecto. Esta es la lista con las librerías necesarias:**\n",
    "\n",
    " - **MediaPipe:** ofrece soluciones de ML como el reconocimiento facial, mallas faciales, holístico, poses, detección de objetos, Motion Tracking etc. Para nuestra propuesta, utilizaremos el holístico, lo que nos permitirá reconocer en vivo de la pose humana simultánea , los puntos de referencia faciales y el seguimiento de mano. Obtendremos puntos coordenadas numéricas de articulaciones los cuales guardaremos en un .csv para su posterior tratamiento. Se puede apreciar mejor en este .gif obtenido de https://google.github.io/mediapipe/solutions/holistic.html.\n",
    "\n",
    "<div style=\"float: centre; width: 100%;\">\n",
    "<img src=\"https://google.github.io/mediapipe/images/mobile/holistic_sports_and_gestures_example.gif\" align=\"centre\">\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **OpenCV:** Lo utilizaremos para obtener, renderizar y procesar la imagen de la cámara.\n",
    " - **Pandas:** Para el tratamiento de los datos obtenidos.\n",
    " - **Scikit-learn:** Para llevar adelante todos los modelos de ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mediapipe opencv-python pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Nota: a día 20/6/2022, existe un problema con las versiones de protobuf cuando se quiere cargar mediapipe. Para solucionarlo, se debe cargar la versión anterior con *!pip install --upgrade protobuf==3.20.1*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade protobuf==3.20.1\n",
    "import mediapipe as mp    #Importamos mediapipe\n",
    "import cv2                #Importamos opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_drawing = mp.solutions.drawing_utils   #Ayudas para dibujar\n",
    "mp_holistic = mp.solutions.holistic       #Soluciones de Mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ej1\"></a>\n",
    "\n",
    "## 1. REALIZAR ALGUNAS DETECCIONES\n",
    "\n",
    "En este apartado se realizarán algunas detecciones para la comprobación del correcto funcionamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conectamos a la cámara frontal (Webcam)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Iniciamos el modelo holístico\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Se guarda la imagen en frame\n",
    "        ret, frame = cap.read()      \n",
    "        \n",
    "        # Reformatear: BGR -> RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        #Para proteger las imágenes\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        #Aplicamos el process para obtener el modelo holistico:\n",
    "        results = holistic.process(image)               \n",
    "        \n",
    "        # RGB -> BGR para renderizar\n",
    "        image.flags.writeable = True  \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # 1. Face landmarks\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                                 )\n",
    "        \n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "        \n",
    "        # Mostramos la imagen                \n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        # Para salir, pulsamos la tecla 'q'\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa en la imagen siguiente que la detección es correcta.\n",
    "    \n",
    "---\n",
    "\n",
    "<div style=\"float: centre; width: 100%;\">\n",
    "<img src=\"https://i.ibb.co/bRwV3C3/holistic.png\" align=\"centre\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "---\n",
    "Ahora, todas las landmarks se guardan en results. Si se escribe directamente el valor de results, obtendremos '*mediapipe.python.solution_base.SolutionOutputs*'. Pero, results está construido por 4 modelos: face_landmarks, right_hand_landmarks, left_hand_landmarks y pose landmarks. \n",
    "\n",
    "    \n",
    "    \n",
    "---\n",
    "    \n",
    "Todos los landmarks del pose que detecta MediaPipe (33) se recogen en la siguiente imagen:\n",
    "\n",
    "![]()\n",
    "\n",
    "<div style=\"float: centre; width: 100%;\">\n",
    "<img src=\"https://google.github.io/mediapipe/images/mobile/pose_tracking_full_body_landmarks.png\" align=\"centre\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Ahora, si compilamos *results.face_landmarks.landmark[0]*, obtendremos las coordenadas en 3D de la nariz:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.face_landmarks.landmark[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Y podemos obtener los valores x,y,z:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x:\", results.pose_landmarks.landmark[0].x,\"y:\", \n",
    "      results.pose_landmarks.landmark[0].y,\"z:\", \n",
    "      results.pose_landmarks.landmark[0].z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Ahora pasaremos al siguiente apartado para almacenar los datos obtenidos de results en un .csv\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ej2\"></a>\n",
    "\n",
    "    \n",
    "---\n",
    "\n",
    "## 2. CAPTURAR LANDMARKS Y EXPORTAR A ARCHIVO CSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Importaremos 3 librerías para realizar el almacenamiento de datos.\n",
    " - **csv:** nos permitirá trabajar con archivos .csv y escribir nuestros datos en ellos.\n",
    " - **os:** usaremos para guardar los archivos en una carpeta específica. \n",
    " - **numpy:** utilizaremos numpy arrays para procesar la información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos librerías necesarias:\n",
    "import csv\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Queremos cuatro valores de los landmarks: x,y,z,v; siendo v la visibilidad.  El modelo facial no dispone del valor de visibilidad, por lo que para todos los puntos de la cara tendremos un v con valor 0.\n",
    "</div> \n",
    "\n",
    "---\n",
    " \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Vamos a loopear por todos los landmarks, por lo que nos interesa saber cuántos son. Ya hemos visto que para el pose teníamos 33 landmarks. Ahora nos faltará los de la cara.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tenemos 501 coordenadas en total\n",
    "num_coords = len(results.pose_landmarks.landmark)+len(results.face_landmarks.landmark)\n",
    "num_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Clasificaremos los coordenadas de cada landmark con su número de identificación.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#la primera columna servirá de identificación de clase \n",
    "landmarks = ['class']\n",
    "\n",
    "for val in range(1,num_coords+1):\n",
    "    landmarks += ['x{}'.format(val), 'y{}'.format(val),'z{}'.format(val),'v{}'.format(val)]\n",
    "\n",
    "#Mostramos ejemplo:\n",
    "print(landmarks[0:13])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Ahora empezaremos con la exportación al archivo .csv. Escribiremos los nombres de las columnas.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('coords.csv', mode='w', newline='') as f:\n",
    "    csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow(landmarks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Ahora, obtenemos un .csv (en la misma carpeta de este archivo) en el cual tenemos las columnas nombradas. \n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Vamos ahora a recopilar datos. Para ello, necesitamos dos clases, el de cansado y despierto. Para cada clase, recogeremos los datos desde la cámara\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = \"Cansado\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conectamos a la cámara frontal (Webcam)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Iniciamos el modelo holístico\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Se guarda la imagen en frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # #Reformatear: BGR -> RGR\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        #Para proteger las imágenes\n",
    "        image.flags.writeable = False\n",
    "        \n",
    "        #Aplicamos el process para obtener el holistico\n",
    "        results = holistic.process(image)\n",
    "\n",
    "        # RGB -> BGR para renderizar\n",
    "        image.flags.writeable = True  \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # 1. Face landmarks\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                                 )\n",
    "        \n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "  \n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "        \n",
    "        # Exportamos las coordenadas\n",
    "        try:\n",
    "\n",
    "            # Guardamos los datos del pose y face en un np array\n",
    "            # Extraemos Pose landmarks ( el método .flatten() convierte el np array en 1D)\n",
    "            pose = results.pose_landmarks.landmark\n",
    "            pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility]\n",
    "                                      for landmark in pose]).flatten())\n",
    "           \n",
    "            # Extraemos Face landmarks            \n",
    "            face = results.face_landmarks.landmark\n",
    "            face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility]\n",
    "                                      for landmark in face]).flatten())\n",
    "        \n",
    "            row = pose_row + face_row\n",
    "            row.insert(0,class_name)\n",
    "            \n",
    "            #Exportar a CSV\n",
    "            with open('coords.csv',mode='a', newline='') as f:\n",
    "                csv_writer = csv.writer(f, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "                csv_writer.writerow(row)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        #Mostramos la imagen\n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        # Para salir, pulsamos la tecla 'q'\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ej3\"></a>\n",
    "\n",
    "## 3. ENTRENAR MODELO UTILIZANDO Scikit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 CARGAR DATOS CON PANDAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora necesitamos otras dos librerías para entrenar el modelo.\n",
    " - Pandas: para el procesamiento del dataframe (df)\n",
    " - Sklearn: en concreto *train_test_split*, con el cual dividiremos el dataset en dos: uno para el entrenamiento y el segundo para validar los modelos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leemos el archivo .csv\n",
    "df = pd.read_csv(\"coords.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardaremos las variables en X y el target en Y\n",
    "X = df.drop('class',axis=1)\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos el dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 MODELO DE CLASIFICACIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear el modelo, necesitaremos importar varias librerías. Cabe mencionar que, para que no dependamos únicamente de un modelo de ML, se va a realizar 4 pipelines de modelos. \n",
    " - **sklearn.pipeline:** nos permitirá llevar a cabo esos 4 pipelines.\n",
    " - **sklearn.preprocessing:** Para la estandarización.\n",
    " - **sklearn.linear_model:** Para importar los modelos lineales *LogisticRegression* y *RidgeClassifier*.\n",
    " - **sklearn.ensemble:** Para importar modelos de ensemble como *RandomForestClassifier* y *GradientBoostingClassifier*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las funciones necesarias\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizaremos 4 modelos de ML para no depender únicamente de una.\n",
    " - Regresión Logística\n",
    " - Ridge Classifier\n",
    " - Random Forest Classifier\n",
    " - Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos los cuatro pipelines\n",
    "pipelines = {\n",
    "    'lr':make_pipeline(StandardScaler(), LogisticRegression()),\n",
    "    'rc':make_pipeline(StandardScaler(), RidgeClassifier()),\n",
    "    'rf':make_pipeline(StandardScaler(), RandomForestClassifier()),\n",
    "    'gb':make_pipeline(StandardScaler(), GradientBoostingClassifier()),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos los modelos en la biblioteca fit_models\n",
    "fit_models = {}\n",
    "for algo, pipeline in pipelines.items():\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    fit_models[algo] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probamos a predecir X_test con el modelo de Random Forest\n",
    "fit_models['rf'].predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 EVALUACIÓN Y SERIALIZACIÓN DEL MODELO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora es hora de evaluar los modelos construidos. Para ello importaremos:\n",
    "- **sklearn.metrics:** la función *accuracy_score* para obtener la precisión obtenida de cada modelo al predecir los *targets* de X_test.\n",
    "- **pickle:** para la serialización del modelo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "A continuación se predicen las clases de X_test para cada modelo.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predecir las clases de X_test\n",
    "for algo, model in fit_models.items():\n",
    "    yhat = model.predict(X_test)\n",
    "    \n",
    "    #Mostramos las precisiones de cada modelo\n",
    "    print(algo, accuracy_score(y_test, yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Se ve que para los primeros dos (*LogisticRegression*, *RidgeClassifier*), la precisión ha sido del 100%. Para *GradientBoostingClassifier* y *RandomForestClassifier* 99.6%. Por elección propia, utilizaré el *RandomForestClassifier*.\n",
    "</div>\n",
    "\n",
    "---\n",
    "    \n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Se serializa entonces el modelo obtenido en un archivo .pkl:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escribimos el modelo en un archivo .pkl\n",
    "with open('cansado_despierto.pkl', 'wb') as f:\n",
    "    pickle.dump(fit_models['rf'], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.  DETECCIONES Y ALERTA AL USUARIO\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Vamos a ir a la parte final de este proyecto. Realizaremos las detecciones de cansado o despierto con el modelo entrenado. Para ello, volvemos a abrir el modelo serializado.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrimos de nuevo el modelo\n",
    "with open('cansado_despierto.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "De nuevo se abrirá la cámara y con la ayuda de MediaPipe haremos detecciones de los landmarks. El modelo será capaz de predecir el estado del/la usuario/a con esos datos. En la pantalla, vamos a mostrar la clase predicha y su probabilidad.\n",
    "\n",
    "---\n",
    "\n",
    "Cuando la probabilidad de estar cansado supere el 70%, vamos a alertar al usuario que necesita un descanso en un rectángulo rojo, como símbolo de alerta.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conectamos la cámara\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Inicia el modelo holístico\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        \n",
    "        #Se guarda la imagen en frame\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        # BGR -> RGB y seguridad de la imagen.\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False        \n",
    "        \n",
    "        # Detecciones\n",
    "        results = holistic.process(image)\n",
    "        \n",
    "        # RGB -> BGR y seguridad de la imagen.        \n",
    "        image.flags.writeable = True   \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # 1. Face landmarks\n",
    "        mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                                 )\n",
    "        \n",
    "        # 2. Right hand\n",
    "        mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 3. Left Hand\n",
    "        mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "\n",
    "        # 4. Pose Detections\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS, \n",
    "                                 mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4),\n",
    "                                 mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                                 )\n",
    "        # Exporta coordenadas\n",
    "        try:\n",
    "            # Extraemos Pose landmarks\n",
    "            pose = results.pose_landmarks.landmark\n",
    "            pose_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility]\n",
    "                                      for landmark in pose]).flatten())\n",
    "            \n",
    "            # Extraemos Face landmarks\n",
    "            face = results.face_landmarks.landmark\n",
    "            face_row = list(np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility]\n",
    "                                      for landmark in face]).flatten())\n",
    "            \n",
    "            # Concatenamos las filas\n",
    "            row = pose_row+face_row\n",
    "\n",
    "            # Hacemos detecciones\n",
    "            X = pd.DataFrame([row])\n",
    "            \n",
    "            # Predecir la clase\n",
    "            body_language_class = model.predict(X)[0]     \n",
    "            \n",
    "            # Predecir la probabilidad de la clase\n",
    "            body_language_prob = model.predict_proba(X)[0]\n",
    "            \n",
    "            # Recogemos coordenadas de la oreja izquierda para visualizar texto dinámicamente\n",
    "            coords = tuple(np.multiply(\n",
    "                            np.array(\n",
    "                                (results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].x, \n",
    "                                 results.pose_landmarks.landmark[mp_holistic.PoseLandmark.LEFT_EAR].y))\n",
    "                        , [640,480]).astype(int))\n",
    "            \n",
    "            \n",
    "            #Mostramos la clase en la oreja izq.\n",
    "            cv2.rectangle(image, \n",
    "                          (coords[0], coords[1]+5), \n",
    "                          (coords[0]+len(body_language_class)*20, coords[1]-30), \n",
    "                          (245, 117, 16), -1)\n",
    "            cv2.putText(image, body_language_class, coords, \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Status Box\n",
    "            cv2.rectangle(image, (0,0), (250, 60), (245, 117, 16), -1)\n",
    "            \n",
    "            # Mostramos Class\n",
    "            cv2.putText(image, 'CLASS'\n",
    "                        , (95,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, body_language_class.split(' ')[0]\n",
    "                        , (90,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Mostramos Probabilidad\n",
    "            cv2.putText(image, 'PROB'\n",
    "                        , (15,12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "            cv2.putText(image, str(round(body_language_prob[np.argmax(body_language_prob)],2))\n",
    "                        , (10,40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Al estar muy cansado, activamos la alerta de descanso.\n",
    "            if (round(body_language_prob[np.argmax(body_language_prob)],2) > 0.70) \n",
    "            and (body_language_class.split(' ')[0] == \"Cansado\"):\n",
    "                \n",
    "                cv2.rectangle(image, \n",
    "                          (coords[0], coords[1]+5), \n",
    "                          (coords[0]+len('MUY CANSADO')*20, coords[1]-30), \n",
    "                          (0, 0, 255), -1)\n",
    "                \n",
    "                cv2.putText(image, 'MUY CANSADO', coords, \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                \n",
    "                cv2.rectangle(image, (0,0), (550, 60), (0, 0, 255), -1)\n",
    "                \n",
    "                cv2.putText(image, \"ALERTA! NECESITAS DESCANSAR\", (15,50), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)               \n",
    "\n",
    "            \n",
    "        except:\n",
    "            pass\n",
    "                        \n",
    "        cv2.imshow('Raw Webcam Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. CONCLUSIONES y PUNTOS A MEJORAR\n",
    "\n",
    "En este proyecto hemos realizado un detector de cansancio, como aplicación de seguridad para los/as conductores en la carretera. Los resultados obtenidos han sido satisfactorios (se muestran en el siguiente .gif) y podrían mejorarse teniendo más datos entrenados o tomando en cuenta más variables en cuanto al cansancio. Incluso se podría estudiar más clases donde añadiríamos distintos grados de cansancio.\n",
    "\n",
    "<div style=\"float: centre; width: 100%;\">\n",
    "<img src=\"https://github.com/Jokin-Cuesta-Arrillaga/Detector-de-Cansancio-con-MediaPipe-y-OpenCV/blob/main/Imagenes/Cansadodespierto.gif?raw=true\" align=\"centre\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
